{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bcf97f5",
   "metadata": {
    "id": "4bcf97f5"
   },
   "source": [
    "## Time Series Forecasting Workshop, Sep 2022\n",
    "\n",
    "*NUS MSBA 2022, Sept 17th*\n",
    "\n",
    "*By Shivam Bansal*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Run this Notebook\n",
    "\n",
    "**Option1:**\n",
    "\n",
    "- Preferred for Mac / Linux Users. \n",
    "\n",
    "- Directly in your Laptop: This notebook can be run directly in your laptop, but need to ensure that required packages are installed. \n",
    "\n",
    "**Option2:**\n",
    "\n",
    "- Run in Google Collab. (Better Option for Windows Users)\n",
    "\n",
    "   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googlecolab/colabtools/blob/masternotebooks/colab-github-demo.ipynb)\n",
    "\n",
    "\n",
    "### Install All Packages \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b19cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Uncomment the following line to install all the required packages used in this notebook ####\n",
    "\n",
    "# !pip3 install matplotlib pandas numpy seaborn statsmodels sklearn xgboost scipy pmdarima prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551d80b3",
   "metadata": {},
   "source": [
    "### 1. Import the Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7aaac",
   "metadata": {
    "id": "d5d7aaac"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.api import SimpleExpSmoothing, ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.interpolate import interp1d\n",
    "import xgboost as xgb\n",
    "import pmdarima as pm\n",
    "from prophet import Prophet\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from pandas import Series\n",
    "import seaborn as sns \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db3fb0c",
   "metadata": {
    "id": "1db3fb0c"
   },
   "source": [
    "### 2. Load Dataset and PreProcessing\n",
    "\n",
    "Monthly Passengers Data from 1949 to 1960 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1941495f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "1941495f",
    "outputId": "a300742b-9f61-4863-fe1f-27b0752d9b33"
   },
   "outputs": [],
   "source": [
    "#### Uncomment and Add Code ####\n",
    "# df = pd.read_csv(\"\")\n",
    "print (\"df Shape: \", df.shape)\n",
    "\n",
    "#### Uncomment and Add Code ####\n",
    "# date_column, target_column = '', ''\n",
    "\n",
    "print (\"df Sample:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43e306",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 618
    },
    "id": "3e43e306",
    "outputId": "3dd47ada-90bc-4d5e-fcf7-dbe93c7a3776"
   },
   "outputs": [],
   "source": [
    "## 2.1 Convert date_column into proper datetime format \n",
    "\n",
    "#### Uncomment and Add Code ####\n",
    "# df[date_column] = <FUNCTION>(df[date_column])\n",
    "\n",
    "## 2.2 Visualize the dataset \n",
    "def line_chart(x_col, y_col, df, title = \"\"):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.title(title);\n",
    "    sns.lineplot(x = x_col, y = y_col, data = df, color = 'green')\n",
    "    plt.show();\n",
    "    \n",
    "line_chart(x_col = date_column, y_col = target_column, df = df, title = \"Monthly Airline Passengers\")\n",
    "\n",
    "## 2.3 Split the dataset - Training and Test Set\n",
    "train_df, test_df = df[:108], df[108:]\n",
    "print (\"Training Set Shape: \", train_df.shape)\n",
    "print (\"Test Set Shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ae8ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "396ae8ae",
    "outputId": "6d33c8b2-65d4-4f32-ab3a-57f5befb5dbe"
   },
   "outputs": [],
   "source": [
    "## 2.4 Missing Values - Detection and Imputation Techniques \n",
    "\n",
    "df_o = df.copy(deep = True)\n",
    "df_o = df_o.set_index(date_column, inplace = False)\n",
    "\n",
    "df_m = pd.read_csv('data/passengers_with_missing.csv')\n",
    "df_m[date_column] = pd.to_datetime(df_m[date_column], infer_datetime_format = True)\n",
    "df_m = df_m.set_index(date_column, inplace = False)\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, sharex=True, figsize=(15, 18))\n",
    "plt.rcParams.update({'xtick.bottom' : False})\n",
    "\n",
    "## 1. Actual \n",
    "df_o.plot(title='Actual', ax=axes[0], label='Actual', color='red', style=\".-\")\n",
    "df_m.plot(title='Actual', ax=axes[0], label='Actual', color='green', style=\".-\")\n",
    "axes[0].legend([\"Missing Data\", \"Available Data\"])\n",
    "\n",
    "## 2. Forward Fill \n",
    "# df_ffill = df_m.ffill()\n",
    "# df_ffill[target_column].plot(title='Forward Fill', ax=axes[1], label='Forward Fill', style=\".-\")\n",
    "\n",
    "## 3. Backward Fill \n",
    "# df_bfill = df_m.bfill()\n",
    "# df_bfill[target_column].plot(title=\"Backward Fill\", ax=axes[2], label='Backward Fill', color='firebrick', style=\".-\")\n",
    "\n",
    "## 4. Linear Interpolation \n",
    "# df_m['row_num'] = np.arange(df_o.shape[0])\n",
    "# df_nona = df_m.dropna(subset = [target_column])\n",
    "# f = interp1d(df_nona['row_num'], df_nona[target_column])\n",
    "# df_m['linear_fill'] = f(df_m['row_num'])\n",
    "# df_m['linear_fill'].plot(title=\"Linear Fill\", ax=axes[3], label='Cubic Fill', color='brown', style=\".-\")\n",
    "\n",
    "## 5. Cubic Interpolation \n",
    "# f2 = interp1d(df_nona['row_num'], df_nona[target_column], kind='cubic')\n",
    "# df_m['cubic_fill'] = f2(df_m['row_num'])\n",
    "# df_m['cubic_fill'].plot(title=\"Cubic Fill\", ax=axes[4], label='Cubic Fill', color='red', style=\".-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237a60db",
   "metadata": {
    "id": "237a60db"
   },
   "source": [
    "### 3. Decomposition - Time Series Components \n",
    "\n",
    "Additive: \n",
    "y(t) = Level + Trend + Seasonality + Noise\n",
    "\n",
    "Multiplicative: \n",
    "y(t) = Level * Trend * Seasonality * Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bea155",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "10bea155",
    "outputId": "6af01ace-9255-4715-80f0-5b14c507267d"
   },
   "outputs": [],
   "source": [
    "#### Uncomment and Add Code ####\n",
    "# series = list(df[target_column])\n",
    "# result = seasonal_decompose(series, model='', period=?)\n",
    "\n",
    "fig = result.plot()\n",
    "fig.set_size_inches((14, 8))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d734d89f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "d734d89f",
    "outputId": "4014f7c9-2c71-4d4f-d7bd-271ac9f2ab3c"
   },
   "outputs": [],
   "source": [
    "#### Uncomment and Add Code ####\n",
    "# series = list(df[target_column])\n",
    "# result = seasonal_decompose(series, model='', period=?)\n",
    "\n",
    "fig = result.plot()\n",
    "fig.set_size_inches((14, 8))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9aeb3",
   "metadata": {
    "id": "2bb9aeb3"
   },
   "source": [
    "### 4. Model - Time Series Forecasting \n",
    "\n",
    "### 4.1 Model1: Simple Moving Average "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241b595",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "f241b595",
    "outputId": "8abd9a47-92a3-4ae9-b5df-9346f69c1446"
   },
   "outputs": [],
   "source": [
    "#### Uncomment and Add Code ####\n",
    "# window = ?\n",
    "# rolling_mean = df[target_column].rolling(window=window).mean()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title(\"Moving Average, window size = \" + str(window))\n",
    "plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "plt.plot(series[window:], label=\"Actual values\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b309a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9b309a2",
    "outputId": "722658c9-a287-46cd-8587-919fea0274af"
   },
   "outputs": [],
   "source": [
    "## Prediction at T = Mean (t-1, t-2, t-3, ... t-n)\n",
    "\n",
    "mean_val = 0\n",
    "for i in range(window):\n",
    "    mean_val += df[target_column].iloc(0)[108-i] \n",
    "    \n",
    "print (mean_val / window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd2e0d3",
   "metadata": {
    "id": "7cd2e0d3"
   },
   "source": [
    "### 4.2 Model2: Simple Exponential Smoothing\n",
    "\n",
    "Weighted averages - Recent observation has more weight than past observations. \n",
    "\n",
    "Exponential smoothing forecasting methods are similar in that a prediction is a weighted sum of past observations, but the model explicitly uses an exponentially decreasing weight for past observations.\n",
    "\n",
    "\n",
    "smoothing parameter: α\n",
    "- When α = 0, the forecasts are equal to the average of the historical data \n",
    "- When α = 1, the forecasts will be equal to the value of the last observation\n",
    "\n",
    "Large values mean that the model pays attention mainly to the most recent past observations, whereas smaller values mean more of the history is taken into account when making a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f394a309",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "f394a309",
    "outputId": "4ce0b286-4ddb-4e48-d032-c745ea7d7628"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(train_df[target_column], marker='o', color=\"black\")\n",
    "\n",
    "fit1 = SimpleExpSmoothing(train_df[target_column]).fit(smoothing_level=0.1, optimized=False)\n",
    "plt.plot(fit1.fittedvalues, marker=\"o\", color=\"b\")\n",
    "\n",
    "fit2 = SimpleExpSmoothing(train_df[target_column]).fit(smoothing_level=0.5, optimized=False)\n",
    "plt.plot(fit2.fittedvalues, marker=\"o\", color=\"r\")\n",
    "\n",
    "fit3 = SimpleExpSmoothing(train_df[target_column]).fit(smoothing_level=1, optimized=False)\n",
    "plt.plot(fit3.fittedvalues, marker=\"o\", color=\"g\")\n",
    "\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a723b8d",
   "metadata": {
    "id": "8a723b8d",
    "outputId": "0e02d8a6-75f7-4048-b728-c38f214b9ab1"
   },
   "outputs": [],
   "source": [
    "fit1.predict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca341d22",
   "metadata": {
    "id": "ca341d22"
   },
   "source": [
    "### Model3: 4.3 Holt Linear\n",
    "\n",
    "Expanding the SES method, the Holt method helps you forecast time series data that has a trend. In addition to the level smoothing parameter α introduced with the SES method, the Holt method adds the trend smoothing parameter β*. Like with parameter α, the range of β* is also between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118d777",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "b118d777",
    "outputId": "25632998-1694-4662-82f3-29a4a35b9dca"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(train_df[target_column], marker='o', color=\"black\")\n",
    "\n",
    "fit1 = ExponentialSmoothing(train_df[target_column]).fit(smoothing_level=0.1, smoothing_slope=0.1, optimized=False)\n",
    "plt.plot(fit1.fittedvalues, marker=\"o\", color=\"b\")\n",
    "\n",
    "fit2 = ExponentialSmoothing(train_df[target_column]).fit(smoothing_level=0.5, smoothing_slope=0.5, optimized=False)\n",
    "plt.plot(fit2.fittedvalues, marker=\"o\", color=\"r\")\n",
    "\n",
    "fit3 = ExponentialSmoothing(train_df[target_column]).fit(smoothing_level=1, smoothing_slope=0.1, optimized=False)\n",
    "plt.plot(fit3.fittedvalues, marker=\"o\", color=\"g\")\n",
    "\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e32bb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "f4e32bb7",
    "outputId": "0e91e7aa-954b-4d92-afae-4ebfe784d1bb"
   },
   "outputs": [],
   "source": [
    "fit1 = Holt(train_df[target_column]).fit()  \n",
    "fit2 = Holt(train_df[target_column], exponential=True).fit() \n",
    "\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.plot(train_df[target_column], marker='o', color='black')\n",
    "plt.plot(fit1.fittedvalues, marker='o', color='b')\n",
    "plt.plot(fit2.fittedvalues, marker='o', color='r')\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f4ffe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be5f4ffe",
    "outputId": "a8304832-ca38-46e2-be6b-d39c653ad75f"
   },
   "outputs": [],
   "source": [
    "fit1.predict()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22ec5a2",
   "metadata": {
    "id": "b22ec5a2"
   },
   "source": [
    "### Model4: 4.4 Holt Winter with Seasonal Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bc9db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "id": "4b8bc9db",
    "outputId": "d8f759d1-cba7-4e55-ebf7-58a961077351"
   },
   "outputs": [],
   "source": [
    "def holt_win_sea(y,y_to_train,y_to_test,seasonal_type,seasonal_period,predict_date):\n",
    "    \n",
    "    y.plot(marker='o', color='black', legend=True, figsize=(14, 7))\n",
    "    \n",
    "    if seasonal_type == 'additive':\n",
    "        fit1 = ExponentialSmoothing(y_to_train, seasonal_periods = seasonal_period, trend='add', seasonal='add').fit()\n",
    "        fcast1 = fit1.forecast(predict_date).rename('Additive')\n",
    "        mse1 = ((fcast1 - y_to_test) ** 2).mean()\n",
    "        print('The RMSE of additive trend, additive seasonal of period season_length={}'.format(seasonal_period))\n",
    "        \n",
    "        fit2 = ExponentialSmoothing(y_to_train, seasonal_periods = seasonal_period, trend='add', seasonal='add', damped=True).fit()\n",
    "        fcast2 = fit2.forecast(predict_date).rename('Additive+damped')\n",
    "        mse2 = ((fcast2 - y_to_test) ** 2).mean()\n",
    "        print('The RMSE of additive damped trend, additive seasonal of period season_length={}'.format(seasonal_period))\n",
    "\n",
    "        fit1.fittedvalues.plot(style='--', color='red')\n",
    "        fcast1.plot(style='--', marker='o', color='red', legend=True)\n",
    "        fit2.fittedvalues.plot(style='--', color='green')\n",
    "        fcast2.plot(style='--', marker='o', color='green', legend=True)\n",
    "    \n",
    "    elif seasonal_type == 'multiplicative':  \n",
    "        fit3 = ExponentialSmoothing(y_to_train, seasonal_periods = seasonal_period, trend='add', seasonal='mul').fit()\n",
    "        fcast3 = fit3.forecast(predict_date).rename('Multiplicative')\n",
    "        mse3 = ((fcast3 - y_to_test) ** 2).mean()\n",
    "        print('The RMSE of multiplicative trend, multiplicative seasonal of period season_length={}'.format(seasonal_period))\n",
    "        \n",
    "        fit4 = ExponentialSmoothing(y_to_train, seasonal_periods = seasonal_period, trend='add', seasonal='mul', damped=True).fit()\n",
    "        fcast4 = fit4.forecast(predict_date).rename('Multiplicative+damped')\n",
    "        mse4 = ((fcast3 - y_to_test) ** 2).mean()\n",
    "        print('The RMSE of multiplicative trend, multiplicative damped seasonal of period season_length={}'.format(seasonal_period))\n",
    "        \n",
    "        fit3.fittedvalues.plot(style='--', color='red')\n",
    "        fcast3.plot(style='--', marker='o', color='red', legend=True)\n",
    "        fit4.fittedvalues.plot(style='--', color='green')\n",
    "        fcast4.plot(style='--', marker='o', color='green', legend=True)\n",
    "        \n",
    "    else:\n",
    "        print('Wrong Seasonal Type. Please choose between additive and multiplicative')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "y = df[target_column]\n",
    "train_y = train_df[target_column]\n",
    "test_y = test_df[target_column]\n",
    "predict_date = len(y) - len(test_y)\n",
    "    \n",
    "#### Uncomment and Add Code ####\n",
    "# holt_win_sea(y, train_y, test_y,'?', ?, predict_date)\n",
    "# holt_win_sea(y, train_y, test_y,'?', ?, predict_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5eaf2",
   "metadata": {
    "id": "57e5eaf2"
   },
   "source": [
    "### Model #5: ARIMA \n",
    "\n",
    "Auto Regressive Integrated with Moving Average \n",
    "\n",
    "---\n",
    "\n",
    "- AR(p) Autoregression – a regression model that utilizes the dependent relationship between a current observation and observations over a previous period. An auto regressive (AR(p)) component refers to the use of past values in the regression equation for the time series\n",
    "\n",
    "- I(d) Integration – uses differencing of observations (subtracting an observation from observation at the previous time step) in order to make the time series stationary. Differencing involves the subtraction of the current values of a series with its previous values d number of times\n",
    "\n",
    "- MA(q) Moving Average – a model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations. A moving average component depicts the error of the model as a combination of previous error terms. The order q represents the number of terms to be included in the model\n",
    "\n",
    "---\n",
    "\n",
    "- p is the order of the AR term\n",
    "- q is the order of the MA term\n",
    "- d is the differencing steps required to make the time series stationary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1cc2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cf1cc2a",
    "outputId": "bca6f3c8-730e-4ad3-ab98-f47f4a89db10"
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "#### Uncomment and Add Code ####\n",
    "# model4 = ARIMA(train_df[target_column].values, order=(?, ?, ?))\n",
    "model4 = model4.fit()\n",
    "print (model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51df5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "3e51df5b",
    "outputId": "15b8e57e-d22d-455a-f8b6-4c2d9a1275eb"
   },
   "outputs": [],
   "source": [
    "residuals = pd.DataFrame(model4.resid)\n",
    "fig, ax = plt.subplots(1,2, figsize=(15, 4))\n",
    "residuals.plot(title=\"Residuals\", ax=ax[0])\n",
    "residuals.plot(kind='kde', title='Density', ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68651312",
   "metadata": {
    "id": "68651312"
   },
   "source": [
    "### Auto ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37442a82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37442a82",
    "outputId": "fd7a2962-7054-4029-f708-86c229f79844"
   },
   "outputs": [],
   "source": [
    "# more details here.- https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html\n",
    "\n",
    "model4_1 = pm.auto_arima(train_df[target_column].values, start_p=1, start_q=1,\n",
    "                      test='adf',       # use adftest to find optimal 'd'\n",
    "                      max_p=3, max_q=3, # maximum p and q\n",
    "                      m=1,              # frequency of series\n",
    "                      d=None,           # let model determine 'd'\n",
    "                      seasonal=False,   # No Seasonality\n",
    "                      start_P=0, \n",
    "                      D=0, \n",
    "                      trace=True,\n",
    "                      error_action='ignore',  \n",
    "                      suppress_warnings=True, \n",
    "                      stepwise=True)\n",
    "\n",
    "print(model4_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15298a9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "15298a9a",
    "outputId": "947ed41a-8390-4084-9192-7a3c326b426f"
   },
   "outputs": [],
   "source": [
    "model4_1.plot_diagnostics(figsize=(10,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8b0dca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb8b0dca",
    "outputId": "231ee82f-e740-4138-92da-dd3b5f99acde"
   },
   "outputs": [],
   "source": [
    "#### Uncomment and Add Code ####\n",
    "# model4_2 = ARIMA(train_df[target_column].values, order=(?,?,?))\n",
    "model4_2 = model4_2.fit()\n",
    "print(model4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce21c9",
   "metadata": {
    "id": "b7ce21c9"
   },
   "outputs": [],
   "source": [
    "start = len(train_df)\n",
    "end  = len(train_df) + len(test_df) -1\n",
    "\n",
    "test_pred = model4_2.predict(start=start, end=end, dynamic=False, typ=\"levels\")\n",
    "test_df['pred'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db574c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "84db574c",
    "outputId": "d752ba57-e367-4bad-e951-b7041b84e1a1"
   },
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(test_df[target_column], test_df[\"pred\"])\n",
    "print('ARIMA Model Test MAE: %.3f' % mae)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Model - ARIMA - Time Series Forecasting\");\n",
    "sns.lineplot(x = date_column, y = target_column, data = df, color = 'green', label='Actual')\n",
    "sns.lineplot(x = date_column, y = 'pred', data = test_df, color = 'red', label = 'Test Predictions')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44033ef9",
   "metadata": {
    "id": "44033ef9"
   },
   "source": [
    "### Model #6: Facebook Prophet\n",
    "\n",
    "1. Open Source time series forecasting model developed by Facebook\n",
    "2. Based on an Additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. \n",
    "3. Works best with time series that have strong seasonal effects and several seasons of historical data\n",
    "4. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n",
    "5. Mainly built for univariate time series models . though there are extensions avialable \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2f6de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "caf2f6de",
    "outputId": "28b46143-a73f-4cb5-b29c-e516bfc51e84"
   },
   "outputs": [],
   "source": [
    "## prepare data for prophet model \n",
    "data_for_prophet = train_df[[date_column, target_column]].rename(columns={date_column: \"ds\", target_column: \"y\"})\n",
    "data_for_prophet_test = test_df[[date_column, target_column]].rename(columns={date_column: \"ds\"})\n",
    "\n",
    "## fit the prophet model \n",
    "model5 = Prophet()\n",
    "model5.fit(data_for_prophet)\n",
    "\n",
    "## predict on train \n",
    "forecast = model5.predict(data_for_prophet)\n",
    "train_pred = forecast.yhat.values\n",
    "train_df['pred'] = train_pred\n",
    "\n",
    "## predict on test\n",
    "forecast = model5.predict(data_for_prophet_test)\n",
    "test_pred = forecast.yhat.values\n",
    "test_df['pred'] = test_pred\n",
    "\n",
    "## Evaluate Model Performance \n",
    "mae = mean_absolute_error(train_df[target_column], train_df[\"pred\"])\n",
    "print('Prophet Model Train MAE: %.3f' % mae)\n",
    "\n",
    "mae = mean_absolute_error(test_df[target_column], test_df[\"pred\"])\n",
    "print('Prophet Model Test MAE: %.3f' % mae)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.title(\"Model - Prophet - Time Series Forecasting\");\n",
    "sns.lineplot(x = date_column, y = target_column, data = df, color = 'green', label='Actual')\n",
    "sns.lineplot(x = date_column, y = 'pred', data = train_df, color = 'blue', label='Train Predictions')\n",
    "sns.lineplot(x = date_column, y = 'pred', data = test_df, color = 'red', label = 'Test Predictions')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455ad40",
   "metadata": {
    "id": "f455ad40"
   },
   "source": [
    "### Model #7: Machine Learning Approach \n",
    "\n",
    "Time Series forecasting problems can be structured as a machine learning (regression) task by adding elements / features of time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WiZ_sT7Cc3tJ",
   "metadata": {
    "id": "WiZ_sT7Cc3tJ"
   },
   "outputs": [],
   "source": [
    "energy_hourly = pd.read_csv('data/energy_consumption.csv', index_col=[0], parse_dates=[0])\n",
    "energy_hourly.sort_index(inplace=True)\n",
    "target_column = 'PJMW_MW'\n",
    "\n",
    "def split_data(data, split_date):\n",
    "    return data[data.index <= split_date].copy(), \\\n",
    "           data[data.index >  split_date].copy()\n",
    "\n",
    "train, test = split_data(energy_hourly, '01-Jan-2015')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('energy consumed')\n",
    "plt.plot(train.index, train[target_column])\n",
    "plt.plot(test.index, test[target_column])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ckGaT0Tdht8",
   "metadata": {
    "id": "9ckGaT0Tdht8"
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Creates time series features from datetime index\n",
    "    \"\"\"\n",
    "    df['date']       = df.index\n",
    "    df['hour']       = df['date'].dt.hour\n",
    "    df['dayofweek']  = df['date'].dt.dayofweek\n",
    "    df['quarter']    = df['date'].dt.quarter\n",
    "    df['month']      = df['date'].dt.month\n",
    "    df['year']       = df['date'].dt.year\n",
    "    df['dayofyear']  = df['date'].dt.dayofyear\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    df['weekofyear'] = df['date'].dt.weekofyear\n",
    "    \n",
    "    X = df[['hour','dayofweek','quarter','month','year','dayofyear','dayofmonth','weekofyear']]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZwKOaKG6dlUM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZwKOaKG6dlUM",
    "outputId": "980d394f-11b8-48ab-ae5c-f878c4815607"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = create_features(train), train[target_column]\n",
    "X_test, y_test   = create_features(test), test[target_column]\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96MS7XmldpYc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96MS7XmldpYc",
    "outputId": "91e2d7fb-c47b-48c0-f2d3-6c66cd099076"
   },
   "outputs": [],
   "source": [
    "reg = xgb.XGBRegressor(n_estimators=1000)\n",
    "\n",
    "reg.fit(X_train, y_train,\n",
    "        eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "        early_stopping_rounds=50, \n",
    "        verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = reg.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, X_test_pred)\n",
    "print('xgBoost Model Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15, 5))\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('energy consumed')\n",
    "plt.plot(y_test.index, y_test)\n",
    "plt.plot(X_test.index, X_test_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97abbbe",
   "metadata": {},
   "source": [
    "### Model #8 - Deep Learning - LSTM\n",
    "\n",
    " \n",
    "In this example we wish to make forcasts on a  time series of international airline passengers.\n",
    " \n",
    "The time series data forcast can be modeled as a univariate regression-type problem, concretely let ${X_t}$ denote the number of airline passengers in month $t$. Then: \n",
    " \n",
    "$$\n",
    "X_t = f(X_{t-1}, \\Theta)\n",
    "$$\n",
    " \n",
    "which we aim to solve using the a simple LSTM neural network. \n",
    "\n",
    "Here $X_t$ is the number of passengers at time step $t$, $X_{t-1}$ denotes  number of passengers at the previous time step, and $\\Theta$ refers to all the other model parameters, including LSTM hyperparameters.\n",
    "\n",
    "*Note*: For better readability, in the code for this as well as the next example, the predicted new value at time step $t$ is written as `Y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f3f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Original data set retrieved from here:\n",
    "# https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&display=line\n",
    "\n",
    "data = pd.read_csv(\"data/international-airline-passengers.csv\", \n",
    "                      usecols = [1], \n",
    "                      engine = \"python\", \n",
    "                      skipfooter = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a35b82",
   "metadata": {},
   "source": [
    "Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d75005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(data, label = \"Airline Passengers\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"1000 International Airline Passengers\")\n",
    "plt.title(\"Monthly Total Airline Passengers 1949 - 1960\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c9e3a",
   "metadata": {},
   "source": [
    "Building LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c907e714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the required libs.\n",
    "# We'll be using the Tensorflow backend (default).\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw data values from the pandas data frame.\n",
    "data_raw = data.values.astype(\"float32\")\n",
    "\n",
    "# We apply the MinMax scaler from sklearn\n",
    "# to normalize data in the (0, 1) interval.\n",
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "dataset = scaler.fit_transform(data_raw)\n",
    "\n",
    "# Print a few values.\n",
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc00489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 60% of data for training, 40% for validation.\n",
    "TRAIN_SIZE = 0.60\n",
    "\n",
    "train_size = int(len(dataset) * TRAIN_SIZE)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\n",
    "print(\"Number of entries (training set, test set): \" + str((len(train), len(test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15406db4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a6f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: This helper function should be rewritten using numpy's shift function. See below.\n",
    "def create_dataset(dataset, window_size = 1):\n",
    "    data_X, data_Y = [], []\n",
    "    for i in range(len(dataset) - window_size - 1):\n",
    "        a = dataset[i:(i + window_size), 0]\n",
    "        data_X.append(a)\n",
    "        data_Y.append(dataset[i + window_size, 0])\n",
    "    return(np.array(data_X), np.array(data_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and training sets for one-step-ahead regression.\n",
    "window_size = 1\n",
    "train_X, train_Y = create_dataset(train, window_size)\n",
    "test_X, test_Y = create_dataset(test, window_size)\n",
    "print(\"Original training data shape:\")\n",
    "print(train_X.shape)\n",
    "\n",
    "# Reshape the input data into appropriate form for Keras.\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(\"New training data shape:\")\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd9cee",
   "metadata": {},
   "source": [
    "The LSTM architecture here consists of:\n",
    "\n",
    "- One input layer.\n",
    "- One LSTM layer of 4 blocks.\n",
    "- One Dense layer to produce a single output.\n",
    "- Use MSE as loss function.\n",
    "- Many different architectures could be considered. But this is just a quick test, so we'll keep things nice and simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f03f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(train_X, train_Y, window_size = 1):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(4, \n",
    "                   input_shape = (1, window_size)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss = \"mean_squared_error\", \n",
    "                  optimizer = \"adam\")\n",
    "    model.fit(train_X, \n",
    "              train_Y, \n",
    "              epochs = 100, \n",
    "              batch_size = 1, \n",
    "              verbose = 2)\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "# Fit the first model.\n",
    "model1 = fit_model(train_X, train_Y, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1369602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_score(model, X, Y):\n",
    "    # Make predictions on the original scale of the data.\n",
    "    pred = scaler.inverse_transform(model.predict(X))\n",
    "    # Prepare Y data to also be on the original scale for interpretability.\n",
    "    orig_data = scaler.inverse_transform([Y])\n",
    "    # Calculate RMSE.\n",
    "    score = math.sqrt(mean_squared_error(orig_data[0], pred[:, 0]))\n",
    "    return(score, pred)\n",
    "\n",
    "rmse_train, train_predict = predict_and_score(model1, train_X, train_Y)\n",
    "rmse_test, test_predict = predict_and_score(model1, test_X, test_Y)\n",
    "\n",
    "print(\"Training data score: %.2f RMSE\" % rmse_train)\n",
    "print(\"Test data score: %.2f RMSE\" % rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c4c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with training predictions.\n",
    "train_predict_plot = np.empty_like(dataset)\n",
    "train_predict_plot[:, :] = np.nan\n",
    "train_predict_plot[window_size:len(train_predict) + window_size, :] = train_predict\n",
    "\n",
    "# Add test predictions.\n",
    "test_predict_plot = np.empty_like(dataset)\n",
    "test_predict_plot[:, :] = np.nan\n",
    "test_predict_plot[len(train_predict) + (window_size * 2) + 1:len(dataset) - 1, :] = test_predict\n",
    "\n",
    "# Create the plot.\n",
    "plt.figure(figsize = (15, 5))\n",
    "plt.plot(scaler.inverse_transform(dataset), label = \"True value\")\n",
    "plt.plot(train_predict_plot, label = \"Training set prediction\")\n",
    "plt.plot(test_predict_plot, label = \"Test set prediction\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"1000 International Airline Passengers\")\n",
    "plt.title(\"Comparison true vs. predicted training / test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad83df59",
   "metadata": {},
   "source": [
    "## Thank You \n",
    "\n",
    "Reach out - Shivam Bansal (shivam.bansal@h2o.ai)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee3717197db56dab91ad083a26bef10706ce761f0ab8e349ac843a6f8d1f4192"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
